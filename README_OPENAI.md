# OpenAI Integration - Quick Reference

## ğŸ¯ What's Ready

âœ… **IntentClassifier** - Unmocked, using OpenAI  
âœ… **ReadingRouter** - Unmocked, using OpenAI  
âœ… **Taxonomy Gate** - Limits books to â‰¤12 candidates  
âœ… **Routing Cache** - 1-hour TTL, auto-invalidates  
âœ… **Eval Harness** - 30 test queries  
âœ… **No UX changes** - Backend only

## âš¡ Quick Test

```bash
# Terminal 1: Set API key
export OPENAI_API_KEY="sk-..."
export USE_MOCK_AGENTS=false

# Terminal 2: Test single query
cd alexandria
source venv/bin/activate
python test_routing.py "How do I deal with things I can't control?"

# Terminal 3: Run full eval (30 queries)
python -m app.agents.eval_harness
```

## ğŸ“ New Files

```
app/agents/
â”œâ”€â”€ llm_provider.py       â† OpenAI wrapper with retries
â”œâ”€â”€ taxonomy.py           â† Domain â†’ book_ids mapping
â”œâ”€â”€ routing_cache.py      â† In-memory routing cache
â”œâ”€â”€ intent_classifier.py  â† Updated to use LLM
â”œâ”€â”€ reading_router.py     â† Updated to use LLM + taxonomy
â””â”€â”€ eval_harness.py       â† 30 test queries

Root:
â”œâ”€â”€ test_routing.py       â† Quick single-query test
â”œâ”€â”€ test_openai.sh        â† Bash test script
â”œâ”€â”€ OPENAI_INTEGRATION.md â† Full setup guide
â”œâ”€â”€ DELIVERY_SUMMARY.md   â† Complete deliverables
â””â”€â”€ README_OPENAI.md      â† This file
```

## ğŸ”‘ Environment Variables

```bash
# Required for LLM mode
export OPENAI_API_KEY="sk-..."

# Optional
export OPENAI_MODEL="gpt-4o-mini"  # default
export USE_MOCK_AGENTS="false"     # true = mock, false = LLM
```

## ğŸ§ª Test Commands

```bash
# Mock mode (no API key needed)
USE_MOCK_AGENTS=true python test_routing.py

# Real LLM mode
USE_MOCK_AGENTS=false OPENAI_API_KEY="sk-..." python test_routing.py

# Full eval with 30 queries
./test_openai.sh real    # or just: ./test_openai.sh for mock
```

## ğŸ“Š Check Stats

```python
# Token usage
from app.agents.llm_provider import get_llm_provider
provider = get_llm_provider()
print(provider.get_usage_summary())

# Cache stats
from app.agents.routing_cache import get_routing_cache
cache = get_routing_cache()
print(cache.get_stats())

# Taxonomy coverage
from app.agents.taxonomy import get_taxonomy_coverage
print(get_taxonomy_coverage())
```

## ğŸ¨ How It Works

```
Query: "Why does my boss act like that?"
    â†“
IntentClassifier (LLM)
    â†’ domain="Strategy", subdomain="Political Philosophy"
    â†“
Taxonomy Gate
    â†’ candidates=["book_062ae004ce4a"]  # 48 Laws of Power
    â†“
ReadingRouter (LLM)
    â†’ paths=[{angle="Power dynamics", recs=[...]}]
    â†“
Cache (1 hour)
    â†“
Return to user
```

## ğŸ›¡ï¸ Constraints

âœ… No embeddings  
âœ… No global search  
âœ… Max 12 candidate books  
âœ… Only books in taxonomy  
âœ… 2-4 parallel paths  
âœ… Max 6 recommendations  
âœ… Fail closed (refuse, don't invent)

## ğŸ’¡ Key Features

**LLM Provider:**
- 3 retries with backoff
- 30s timeout
- Token logging
- JSON validation

**Taxonomy:**
- Hardcoded mappings
- Version tracking
- Caps at 12 books

**Cache:**
- 1-hour TTL
- Query normalization
- Auto-invalidation

**Router:**
- Parallel paths (different angles)
- Validates book_ids
- Never hallucinates

## ğŸ“– Read More

- **Setup:** `OPENAI_INTEGRATION.md`
- **Deliverables:** `DELIVERY_SUMMARY.md`
- **Test:** Run `./test_openai.sh`

## ğŸš¨ Important

- **TextCompanion still mocked** (section chat not unmocked yet)
- **No template changes** (UX unchanged)
- **Taxonomy is hardcoded** (update in `taxonomy.py`)
- **Cache is in-memory** (use Redis for production)

---

**Status:** âœ… Ready to test  
**Cost:** ~$0.0008 per query (gpt-4o-mini)  
**Setup time:** 2 minutes

